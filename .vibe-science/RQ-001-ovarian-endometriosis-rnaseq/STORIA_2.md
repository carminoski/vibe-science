Report Tecnico: Migrazione Workflow scRNA-seq (R → Python)1. Contesto e MotivazioniIl ProgettoL'obiettivo è l'analisi di un atlante single-cell RNA-seq comprendente 52 campioni (Endometrio Eutopico, Endometriosi/OMA, Ovaio Sano, e carcinomi ovarici CCOC, EnOC, LGsOC), per un totale di oltre 390.000 cellule. I dati provengono da tecnologie miste (principalmente 10x Genomics) e formati eterogenei scaricati da NCBI GEO.Il Problema (Il "Muro" della RAM in R)Il workflow iniziale, sviluppato in R (Seurat/Monocle3), si è interrotto durante le fasi di integrazione. Nonostante l'utilizzo di un'istanza cloud AWS con 750 GB di RAM, il processo falliva per Out Of Memory (OOM).Causa: R tende a caricare e duplicare le matrici dense in memoria. Operazioni come SCTransform, IntegrateData e la costruzione del grafo dei vicini su ~400k cellule richiedono risorse esponenziali.La Soluzione: Migrazione a Python (Scanpy + scVI)Abbiamo riscritto l'intera pipeline in Python per sfruttare:Sparsità Nativa: AnnData gestisce matrici sparse (CSR), riducendo l'impronta di memoria del ~90%.I/O Efficiente: Caricamento dati lazy e formati compressi (.h5ad, .zarr).Integrazione Scalabile: Adozione di scvi-tools (Deep Learning con mini-batching) che non richiede il caricamento totale dei dati in memoria.2. Fase 1: Data Ingestion & Standardization (Completata)Questa fase è stata la più critica a causa dell'estrema eterogeneità dei dati grezzi. Abbiamo dovuto scrivere un parser universale capace di gestire formati 10x standard, file H5, e file di testo (CSV/TXT) con orientamenti e separatori variabili.Sfide Tecniche e Bug RisoltiDurante lo sviluppo dello script di ingestione, abbiamo incontrato e risolto questi problemi specifici:Dataset Aggregato (GSE111976)Problema: 6 campioni "eutopic" erano forniti in un unico file .rds gigante convertito poi in CSV (4.6 GB).Soluzione: Creato uno script di pre-splitting che, leggendo i metadati, ha separato il CSV gigante in 6 cartelle individuali (data/raw/eutopic_7...12).Nomi File Non Standard (normal_5, normal_6, normal_7)Problema: Questi campioni 10x avevano prefissi nei nomi file (es. GSM5599223_Norm4.matrix.mtx.gz) che il loader standard di Scanpy ignorava, risultando in dataset vuoti (0, 0).Soluzione: Implementato un loader custom che cerca i pattern *matrix.mtx* indipendentemente dal prefisso.Errori "Invalid Integer" (eutopic_5, eutopic_7)Problema: Alcuni file .mtx contenevano valori decimali (float) derivanti da quantificazioni probabilistiche, ma l'header del file dichiarava "integer", mandando in crash la libreria scipy.io.mmread.Soluzione: Implementato un fallback _read_mtx_flexible che parsa il file ignorando l'header errato e forzando la lettura come float.Errori di Immutabilità dell'Indice (CCCOC, LGSOC)Problema: Errori tipo TypeError: Index does not support mutable operations. Tentavamo di modificare adata.var_names (che è un pd.Index immutabile) in-place durante la mappatura dei geni.Soluzione: Riscritta la logica di mappatura Ensembl creando nuove pd.Series o array numpy, modificando quelli, e sovrascrivendo l'indice in un colpo solo.Performance sui CSVProblema: Il caricamento dei CSV testuali era lentissimo.Soluzione: Utilizzo di pd.read_csv(..., engine="c", dtype=np.float32) per massimizzare la velocità e ridurre l'uso di memoria.3. Stato Attuale e Prossimi PassiCon la Fase 1 conclusa, tutti i 52 campioni sono stati convertiti in file .h5ad standardizzati nella cartella work/01_qc_h5ad.Le prossime fasi (già pianificate) sono:Fase 2 (Merge): Unione dei 52 file in un unico oggetto AnnData.Fase 2.5 (Doublet Removal): Pulizia dei doppietti con SOLO/scVI.Fase 3 (Integrazione & Analisi): Integrazione batch-aware con scVI, UMAP con tecnica subsetting, Clustering e identificazione dei marcatori.

